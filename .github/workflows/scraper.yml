name: Daily Data Scraper

on:
  schedule:
    # Runs at 02:00 UTC every day
    - cron: "0 2 * * *"
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest

    permissions:
      # Give the default GITHUB_TOKEN write permission to commit and push the changed files back to the repository.
      contents: write

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip" # caching pip dependencies

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r backend/scraper/requirements.txt

      - name: Run Scraper
        run: |
          cd backend/scraper
          python scraper.py

      - name: Commit and Push Changes
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

          # Only commit if opportunities.json has changed
          git add backend/scraper/opportunities.json

          # git diff-index --quiet HEAD will exit with 1 if there are differences, 0 if none
          if git diff-index --quiet HEAD; then
            echo "No changes in data to commit."
          else
            git commit -m "chore(data): auto-update opportunities.json [skip ci]"
            git push
            echo "Successfully committed newly scraped data!"
          fi
